강화 학습 결과 및 코드 개요

이 저장소에는 MiniGrid-Empty-6x6과 MiniGrid-Crossing 두 환경에서 수행된 강화 학습 실험의 결과와 코드가 포함되어 있습니다. 결과는 각 작업(task)별로 정리되어 있으며, 저장소 구조는 다양한 실험 결과와 해당 코드를 쉽게 탐색할 수 있도록 설계되었습니다.

저장소 구조

empty_6x6/

MiniGrid-Empty-6x6 환경에서 수행된 실험 결과가 포함되어 있습니다.

각 실험의 에이전트 수행 영상, 학습된 Q-값, 보상 그래프가 제공됩니다.

보상 그래프는 에피소드 수에 따라 나뉘어 있어 에이전트의 학습 경과를 비교할 수 있습니다.

crossing/

MiniGrid-Crossing 환경에서 수행된 실험 결과가 포함되어 있습니다.

에이전트 수행 영상, 학습된 Q-값, 보상 그래프가 포함되어 있습니다.

보상 그래프는 다양한 하이퍼파라미터와 에피소드 수에 따른 최적 구성을 비교하여 보여줍니다.

code/

모든 실험에 사용된 소스 코드가 포함되어 있습니다.

각 환경 및 실험 설정에 대한 개별 Python 스크립트가 있으며, SARSA와 Q-learning 구현이 포함되어 있습니다.

내용 개요

Empty 6x6 환경

empty_6x6 폴더에는 다양한 에피소드 수를 사용하여 학습된 강화 학습 에이전트의 시각적 및 수치적 결과가 포함되어 있습니다. 이를 통해 에이전트의 성능이 시간이 지남에 따라 어떻게 발전하는지 비교할 수 있습니다. 다음과 같은 내용이 포함되어 있습니다:

에피소드 보상 그래프: 에피소드가 진행됨에 따라 보상이 어떻게 변화하는지 보여주는 그래프입니다.

수행 영상: 에이전트의 학습된 행동을 시연한 영상입니다.

Q-값: 에이전트의 의사결정 과정을 이해할 수 있도록 학습된 Q-값을 제공합니다.

Crossing 환경

crossing 폴더에는 보다 복잡한 환경에서 학습된 강화 학습 에이전트의 결과가 포함되어 있습니다. 이 실험의 초점은 효과적인 학습을 위한 최적의 하이퍼파라미터를 찾는 것이었습니다. 폴더에는 다음과 같은 내용이 포함되어 있습니다:

하이퍼파라미터 비교: 다양한 하이퍼파라미터 설정 및 에피소드 수의 영향을 보여주는 그래프입니다.

수행 영상: 최적의 설정을 사용했을 때의 에이전트 행동을 시연한 영상입니다.

Q-값: 학습 중 Q-값이 어떻게 발전했는지에 대한 정보입니다.

코드

code 폴더에는 이러한 실험에 사용된 모든 스크립트가 포함되어 있으며, 다음과 같은 내용이 있습니다:

환경 래퍼: 환경을 수정하기 위한 커스텀 래퍼입니다.

SARSA와 Q-learning: MiniGrid 환경에서 에이전트를 학습시키기 위한 SARSA와 Q-learning 알고리즘의 구현이 포함되어 있습니다.

사용 방법

결과를 탐색하려면:

각각의 폴더 (empty_6x6/ 또는 crossing/)로 이동하여 각 환경의 결과를 확인하세요.

코드 실행: code/ 폴더에 있는 스크립트를 사용하여 실험을 재현하거나 설정을 수정하여 추가 탐구를 진행할 수 있습니다.

개선 사항 제안, 새로운 실험 실행, 또는 풀 리퀘스트 제출을 통해 자유롭게 기여해 주세요!
